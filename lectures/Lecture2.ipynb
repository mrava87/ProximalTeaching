{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff085670",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pylops\n",
    "import pyproximal\n",
    "from scipy.signal import filtfilt, convolve\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    " \n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92e1a713",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".output_png {\n",
       "    display: table-cell;\n",
       "    text-align: center;\n",
       "    vertical-align: middle;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62785667",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## An introduction to proximal solvers for engineers\n",
    "### Day 2 @ Luna Innovation \n",
    "### Instructor: Matteo Ravasi, Assistant Professor - KAUST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302f1be1-de91-48ca-bb60-272ca660af86",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Half-Quadratic Splitting (HQS)\n",
    "\n",
    "When both where $f$ and $g$ are non-smooth, convex functions, the Proximal gradient algorithm (or ISTA) cannot be used.\n",
    "\n",
    "A two steps procedure is required:\n",
    "\n",
    "- Splitting: $\\mathbf{y}=\\mathbf{m}$, such that\n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{m}, \\mathbf{y}} {\\mathrm{argmin}} \\quad f(\\mathbf{m}) + g(\\mathbf{y}) \\; s.t. \\; \\mathbf{y}=\\mathbf{m}\n",
    "$$\n",
    "\n",
    "- Relaxation: $\\text{arg} \\underset{\\mathbf{m},\\mathbf{y}} {\\mathrm{min}} \\mathscr{L}$, where \n",
    "$$\n",
    "\\mathscr{L}=f(\\mathbf{m}) + g(\\mathbf{y}) + \\frac{\\rho}{2}||\\mathbf{m}-\\mathbf{y}||_2^2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbe768b-288e-4c23-a3fa-c524585439b3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Half-Quadratic Splitting (HQS)\n",
    "\n",
    "- Alternating minimization:\n",
    "\n",
    "$$\n",
    "\\mathbf{m}_k = \\underset{\\mathbf{m}} {\\mathrm{argmin}} \\; \\mathscr{L}(\\mathbf{m}, \\mathbf{y}_{k-1}) = \\underset{\\mathbf{m}} {\\mathrm{argmin}} \\; f(\\mathbf{m}) + \\frac{\\rho_k}{2}||\\mathbf{m}-\\mathbf{y}_{k-1}||_2^2 = prox_{\\tau_k f}(\\mathbf{y}_{k-1})\n",
    "$$\n",
    "$$\n",
    "\\mathbf{y}_k = \\underset{\\mathbf{y}} {\\mathrm{argmin}} \\; \\mathscr{L}(\\mathbf{m}_k, \\mathbf{y}) = \\underset{\\mathbf{y}} {\\mathrm{argmin}} \\; g(\\mathbf{y}) + \\frac{\\rho_k}{2}||\\mathbf{m}_k-\\mathbf{y}||_2^2 =  prox_{\\tau_k g}(\\mathbf{m}_k)\n",
    "$$\n",
    "\n",
    "where $\\tau_k=1/\\rho_k$. To converge $lim_{k \\rightarrow \\infty} \\rho_k = \\infty$ (or $lim_{k \\rightarrow \\infty} \\tau_k = 0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11756420-aee0-4981-b414-dde59cc8a634",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Half-Quadratic Splitting (HQS)\n",
    "\n",
    "Time to practice: [EX2](https://github.com/mrava87/ProximalTeaching/blob/main/examples/BasisPursuit.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d155194",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## ADMM\n",
    "\n",
    "Similar to HQS, a two steps procedure is required:\n",
    "\n",
    "- Splitting: $\\mathbf{y}=\\mathbf{m}$, such that\n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{m}, \\mathbf{y}} {\\mathrm{argmin}} \\quad f(\\mathbf{m}) + g(\\mathbf{y}) \\; s.t. \\;  \\mathbf{y}=\\mathbf{m}\n",
    "$$\n",
    "\n",
    "- Relaxation by Augmented Lagrangian: $\\text{arg} \\underset{\\mathbf{m},\\mathbf{y}} {\\mathrm{min}} \\underset{\\mathbf{z}}  {\\mathrm{max}} \\mathscr{L}$, where \n",
    "$$\n",
    "\\mathscr{L}=f(\\mathbf{m}) + g(\\mathbf{y}) + \\frac{\\rho}{2}||\\mathbf{m}-\\mathbf{y}||_2^2 + \\mathbf{z}^T(\\mathbf{m}-\\mathbf{y})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf7f9fb-ceb5-479e-a163-a931b91d44e0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## ADMM\n",
    "\n",
    "- Rescaling (scaled form = combine the last two terms):\n",
    "\n",
    "$$\n",
    "\\frac{\\rho}{2}||\\mathbf{m}-\\mathbf{y}||_2^2 + \\mathbf{z}^T(\\mathbf{m}-\\mathbf{y}) = \n",
    "$$\n",
    "$$\n",
    "\\frac{\\rho}{2} ||\\mathbf{m}-\\mathbf{y} + 1/\\rho \\mathbf{z}||_2^2 - 1/(2\\rho) ||\\mathbf{z}||_2^2 = \n",
    "$$\n",
    "$$\n",
    "\\frac{\\rho}{2} ||\\mathbf{m}-\\mathbf{y} + \\mathbf{z}'||_2^2 - \\frac{\\rho}{2} ||\\mathbf{z}'||_2^2\n",
    "$$\n",
    "\n",
    "where $\\mathbf{z}'=\\mathbf{z}/\\rho$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75409920-2ea7-4306-8733-2dd6d31fb083",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ADMM\n",
    "\n",
    "So we are left with:\n",
    "\n",
    "$$\n",
    "\\mathscr{L}=f(\\mathbf{m}) + g(\\mathbf{y}) + \\frac{\\rho}{2} ||\\mathbf{m}-\\mathbf{y} + \\mathbf{z}'||_2^2 - \\frac{\\rho}{2} ||\\mathbf{z}'||_2^2\n",
    "$$\n",
    "\n",
    "\n",
    "where $\\mathbf{z}'=\\mathbf{z}/\\rho$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edaabce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## ADMM\n",
    "\n",
    "- Alternating minimization:\n",
    "\n",
    "$$\n",
    "\\mathbf{m}_k = \\underset{\\mathbf{m}} {\\mathrm{argmin}} \\quad \\mathscr{L}(\\mathbf{m}, \\mathbf{y}_{k-1}, \\mathbf{z}'_{k-1}) = prox_{\\tau_k f}(\\mathbf{y}_{k-1}-\\mathbf{z}'_{k-1})\n",
    "$$\n",
    "$$\n",
    "\\mathbf{y}_k = \\underset{\\mathbf{y}} {\\mathrm{argmin}} \\quad \\mathscr{L}(\\mathbf{m}_k, \\mathbf{y}, \\mathbf{z}'_{k-1}) = prox_{\\tau_k g}(\\mathbf{m}_k+\\mathbf{z}'_{k-1})\n",
    "$$\n",
    "$$\n",
    "\\mathbf{z}'_k = \\underset{\\mathbf{z}'} {\\mathrm{argmax}} \\quad \\mathscr{L}(\\mathbf{m}_k, \\mathbf{y}_k, \\mathbf{z}') = \\mathbf{z}'_{k-1} + \\mathbf{m}_k - \\mathbf{y}_k\n",
    "$$\n",
    "\n",
    "where $\\tau_k=1/\\rho_k \\in (0, 1/L]$ with $L$ being the Lipschitz constant of $\\nabla f$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba49aac4-4a67-4c31-bd62-06b9c5e9fdce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## General ADMM\n",
    "\n",
    "In theory, the ADMM algorithm is much more powerful than that. It can solve any problem of the form:\n",
    "\n",
    "$$\n",
    "\\text{arg} \\underset{\\mathbf{m}} {\\mathrm{min}} \\quad f(\\mathbf{m}) + g(\\mathbf{y}) \\; s.t. \\mathbf{Am}+\\mathbf{By}=c\n",
    "$$\n",
    "Relaxation by Augmented Lagrangian: $\\text{arg} \\underset{\\mathbf{m},\\mathbf{y}} {\\mathrm{min}} \\underset{\\mathbf{z}}  {\\mathrm{max}} \\mathscr{L}$, where \n",
    "$$\n",
    "\\mathscr{L}=f(\\mathbf{m}) + g(\\mathbf{y}) + \\frac{\\rho}{2}||\\mathbf{Am}+\\mathbf{By}-c||_2^2 + \\mathbf{z}^T(\\mathbf{Am}+\\mathbf{By}-c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd30c4b6-6bb1-4078-9aa0-98b647478eab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## General ADMM\n",
    "\n",
    "We need to define some special cases to find useful solutions:\n",
    "\n",
    "1. $\\mathbf{A}=\\mathbf{I}$, $\\mathbf{B}=-\\mathbf{I}$, $c=0$: see above.\n",
    "2. $\\mathbf{A}=\\mathbf{L}$, $\\mathbf{B}=-\\mathbf{I}$, $c=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcc7612",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## ADMM (case 2)\n",
    "\n",
    "When $g(\\mathbf{Lm})$ (e.g., $||\\mathbf{Lm}||_p$), we follow similar to optimize the associated functional:\n",
    "\n",
    "$$\n",
    "\\text{arg} \\underset{\\mathbf{m}} {\\mathrm{min}} \\quad f(\\mathbf{m}) + g(\\mathbf{Lm})\n",
    "$$\n",
    "\n",
    "- Splitting: $\\mathbf{y}=\\mathbf{Lm}$\n",
    "- Augmented Lagrangian: $\\text{arg} \\underset{\\mathbf{m},\\mathbf{y}} {\\mathrm{min}} \\underset{\\mathbf{z}}  {\\mathrm{max}} \\mathscr{L}$, where \n",
    "$$\n",
    "\\mathscr{L}=f(\\mathbf{m}) + g(\\mathbf{y}) + \\frac{\\rho}{2}||\\mathbf{Lm}-\\mathbf{y}||_2^2 + \\mathbf{z}^T(\\mathbf{Lm}-\\mathbf{y})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0296b4dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## ADMM (case 2)\n",
    "\n",
    "- Alternating minimization:\n",
    "\n",
    "$$\n",
    "\\mathbf{m}_k = \\underset{\\mathbf{m}} {\\mathrm{argmin}} \\quad \\mathscr{L}(\\mathbf{m}, \\mathbf{y}_{k-1}, \\mathbf{z}_{k-1})\n",
    "$$\n",
    "$$\n",
    "\\mathbf{y}_k = \\underset{\\mathbf{y}} {\\mathrm{argmin}} \\quad \\mathscr{L}(\\mathbf{m}_k, \\mathbf{y}, \\mathbf{z}_{k-1}) = prox_{\\tau_k g}(\\mathbf{Lm}_{k} + \\mathbf{z}_{k-1})\n",
    "$$\n",
    "$$\n",
    "\\mathbf{z}_k = \\underset{\\mathbf{z}} {\\mathrm{argmax}} \\quad \\mathscr{L}(\\mathbf{m}_k, \\mathbf{y}_k, \\mathbf{z}) = \\mathbf{z}_{k-1} + \\mathbf{Lm}_k - \\mathbf{y}_k\n",
    "$$\n",
    "\n",
    "where $\\tau_k=1/\\rho_k \\in (0, 1/\\lambda_{max}(\\mathbf{L}^H \\mathbf{L})]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed7ffd8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## ADMM (case 3)\n",
    "\n",
    "Let's now consider a special case where\n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{m}} {\\mathrm{argmin}} \\quad \\frac{1}{2} ||\\mathbf{d} - \\mathbf{Gm}||_2^2 + g(\\mathbf{Lm})\n",
    "$$\n",
    "\n",
    "This reduces to:\n",
    "\n",
    "$$\n",
    "\\mathbf{m}_k = (\\mathbf{G}^H \\mathbf{G} + \\rho_k \\mathbf{L}^H \\mathbf{L})^{-1} (\\mathbf{G}^H \\mathbf{d} +  \\rho_k \\mathbf{L}^H(\\mathbf{y}_{k-1} - \\mathbf{z}_{k-1}))\n",
    "$$\n",
    "$$\n",
    "\\mathbf{y}_k = prox_{\\tau_k g}(\\mathbf{Lm}_{k} + \\mathbf{z}_{k-1}) \n",
    "$$\n",
    "$$\n",
    "\\mathbf{z}_k = \\mathbf{z}_{k-1} + \\mathbf{Lm}_k - \\mathbf{y}_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cebf45",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## ADMM (case 3)\n",
    "\n",
    "Note that the first step is the proximal operator of the L2 squared norm of the following system of equations:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "\\mathbf{G} \\\\\n",
    "\\sqrt{ \\rho_k}\\mathbf{L}\n",
    "  \\end{bmatrix} \\mathbf{m} = \n",
    "\\begin{bmatrix} \n",
    "\\mathbf{d} \\\\\n",
    "\\sqrt{ \\rho_k}(\\mathbf{y}_{k-1} - \\mathbf{z}_{k-1})\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and can be solved directly or via any of the iterative scheme for least-squares inversion (e.g., CGLS, LSQR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e31ff92-e0c5-4ebc-bbdc-9f1b583340fe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Total-variation (TV) norm\n",
    "\n",
    "A special norm, commonly used in signal and image processing --> renders a blocky solution.\n",
    "\n",
    "Two types (here shown for 2D case):\n",
    "\n",
    "- Anisotropic TV:\n",
    "\n",
    "$$\n",
    "TV_a(\\mathbf{m})=||\\nabla \\mathbf{m}||_1 = \\sum_{i=1}^{N_xN_z} |\\mathbf{m}_x|_i + |\\mathbf{m}_z|_i\n",
    "$$\n",
    " \n",
    "- Isotropic TV:\n",
    "\n",
    "$$\n",
    "TV_i(\\mathbf{m})=||\\nabla \\mathbf{m}||_{2,1} = \\sum_{i=1}^{N_xN_z} \\sqrt{(\\mathbf{m}_x)_i^2 +(\\mathbf{m}_z)_i^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698241af-feef-487a-bdab-377cce0eeb5f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Total-variation (TV) norm\n",
    "\n",
    "Anisotropic TV: $TV_a(\\mathbf{m})=||\\nabla \\mathbf{m}||_1 = \\sum_{i=1}^{N_xN_z} |\\mathbf{m}_x|_i + |\\mathbf{m}_z|_i$ \n",
    "\n",
    "where $\\nabla: \\mathbb{R}^{N_xN_z} \\rightarrow \\mathbb{R}^{2 N_xN_z}$ is the gradient operator, that applies directional derivatives along the x- and z-axes and stacks them vertically: $\\nabla \\mathbf{m} = [\\mathbf{m}_x^T, \\mathbf{m}_z^T]^T$.\n",
    "\n",
    "Can be easily expressed in the form of $g(\\mathbf{Lm})$: $g = ||\\cdot||_1, \\mathbf{L}=\\nabla$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7395dc-3df1-4c7f-8998-85fe6ad553db",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Total-variation (TV) norm\n",
    "\n",
    "Isotropic TV: $TV_i(\\mathbf{m})=||\\nabla \\mathbf{m}||_{2,1} = \\sum_{i=1}^{N_xN_z} \\sqrt{(\\mathbf{m}_x)_i^2 +(\\mathbf{m}_z)_i^2}$ \n",
    "\n",
    "where $\\nabla: \\mathbb{R}^{N_xN_z} \\rightarrow \\mathbb{R}^{2 \\times  N_xN_z}$ is the gradient operator, that applies directional derivatives along the x- and z-axes and stacks them horizontally: $\\nabla \\mathbf{m} = [\\mathbf{m}_x, \\mathbf{m}_z]^T$.\n",
    "\n",
    "Can be easily expressed in the form of $g(\\mathbf{Lm})$: $g = ||\\cdot||_{2,1}, \\mathbf{L}=\\nabla$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b9ff9d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Total-variation (TV) norm\n",
    "\n",
    "Let's take now an example:\n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{m}} {\\mathrm{argmin}} \\quad \\frac{1}{2} ||\\mathbf{d} - \\mathbf{Gm}||_2^2 + \\alpha TV_i(\\mathbf{m}) \n",
    "$$\n",
    "\n",
    "This can be seen as a special case of the ADMM (case 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b68d876-cf62-40a5-baf2-784d7420068a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Total-variation (TV) norm\n",
    "\n",
    "Time to practice: [EX3](https://github.com/mrava87/ProximalTeaching/blob/main/examples/SignalDenoising.ipynb) and [EX4](https://github.com/mrava87/ProximalTeaching/blob/main/examples/MRIReconstruction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d28d2-2e02-480a-9b19-ac90c8c0f040",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Chambolle-Pock Primal Dual\n",
    "\n",
    "A more general solver, able to handle the following objective function\n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{m}} {\\mathrm{argmin}} \\quad f(\\mathbf{m}) + g(\\mathbf{Lm})\n",
    "$$\n",
    "\n",
    "for any choice of $f$ and $g$ proximable functions, and linear operator $\\mathbf{L}$.\n",
    "\n",
    "**Disclaimer: this is my favourite solver.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3598f4-bac3-4c0a-b769-8472cc3f709d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Chambolle-Pock Primal Dual (PD)\n",
    "\n",
    "Why PD is my favourite?\n",
    "\n",
    "- Very flexible in terms of objective function;\n",
    "- In many problems I work with, outperforms other state-of-the-art solvers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f565ec-5475-4a7d-a3ad-b9b47ac95ddf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Chambolle-Pock Primal Dual (PD)\n",
    "\n",
    "PD effectively minimizes the equivalent primal-dual problem (primal in $f$, dual in $g$):\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{m}} \\max_{\\mathbf{y}} \\mathbf{y}^T(\\mathbf{Lm}) + f(\\mathbf{m}) - g^*(\\mathbf{y})\n",
    "$$\n",
    "\n",
    "where $\\mathbf{y}$ is the so-called dual variable, and $g^*$ is the so-called convex conjugate function of $g$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dc94ad-31b5-4290-bf2e-ba2f05e759c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Chambolle-Pock Primal Dual (PD)\n",
    "\n",
    "To compute the proximal operator of a convex conjugate function (i.e., dual-proximal), we simply rely on the *Moreau identity*:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = prox_{\\tau f} (\\mathbf{x}) + \\tau prox_{\\frac{1}{\\tau} f^*} \\left(\\frac{\\mathbf{x}}{\\tau}\\right)\n",
    "$$\n",
    "\n",
    "So i we know how to compute $prox_{\\tau f}$, we can also compute $prox_{\\frac{1}{\\tau} f^*}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d5e4a-9e28-485c-94ba-787580ebb416",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Chambolle-Pock Primal Dual (PD)\n",
    "\n",
    "The derivation is rather complicated, I provide here just the algorithm:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}^{k+1} = prox_{\\mu g^*}(\\mathbf{y}^{k} + \\mu \\mathbf{L}\\bar{\\mathbf{x}}^{k})\n",
    "$$\n",
    "$$\n",
    "\\mathbf{x}^{k+1} = prox_{\\tau f}(\\mathbf{x}^{k} - \\tau \\mathbf{L}^H \\mathbf{z}^{k+1})\n",
    "$$\n",
    "$$\n",
    "\\bar{\\mathbf{x}}^{k+1} = \\mathbf{x}^{k+1} + \\theta (\\mathbf{x}^{k+1} - \\mathbf{x}^k)\n",
    "$$\n",
    "\n",
    "where $\\tau \\mu \\lambda_{max}(\\mathbf{L}^H\\mathbf{L}) < 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a885c28-a371-42ff-a218-06d825a18835",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Chambolle-Pock Primal Dual (PD)\n",
    "\n",
    "Derivation:\n",
    "\n",
    "- Splitting:\n",
    "$$\n",
    "\\text{arg} \\underset{\\mathbf{m}} {\\mathrm{min}} \\quad f(\\mathbf{m}) + g(\\mathbf{y}) \\; s.t. \\mathbf{Lm}=\\mathbf{y}\n",
    "$$\n",
    "\n",
    "- Relaxation by Lagrangian: $\\text{arg} \\underset{\\mathbf{m},\\mathbf{y}} {\\mathrm{min}} \\underset{\\mathbf{z}}  {\\mathrm{max}} \\mathscr{L}$, where \n",
    "$$\n",
    "\\mathscr{L}=f(\\mathbf{m}) + g(\\mathbf{y}) + \\mathbf{z}^T(\\mathbf{Lm}-\\mathbf{y})\n",
    "$$\n",
    "\n",
    "- Use definition of convex conjugate function ($f^{\\star}(\\mathbf{z}) = \\sup_\\mathbf{m} \\{\\mathbf{z}^T\\mathbf{m} - f(\\mathbf{m})\\}$)\n",
    "\n",
    "$$\n",
    "\\text{arg} \\underset{\\mathbf{m}} {\\mathrm{min}} \\underset{\\mathbf{z}}  {\\mathrm{max}} \\; \\mathbf{z}^T\\mathbf{Lm} + f(\\mathbf{m}) - g^{\\star}(\\mathbf{z})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a1c3e-c592-42c9-8bd6-17585301c46e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Chambolle-Pock Primal Dual (PD)\n",
    "\n",
    "Derivation:\n",
    "\n",
    "- Define optimality conditions:\n",
    "$$\n",
    "0 \\in -\\partial g^{\\star}(\\mathbf{z}) + \\mathbf{Lm}\n",
    "$$\n",
    "$$\n",
    "0 \\in \\partial f(\\mathbf{m}) + \\mathbf{L}^T\\mathbf{z}\n",
    "$$\n",
    "\n",
    "- Adding $\\mathbf{m}$ on both sides:\n",
    "$$\n",
    "(\\mathbf{I} + \\partial g^{\\star})(\\mathbf{z}) \\in  \\mathbf{z} + \\mathbf{L}\\mathbf{m}\n",
    "$$\n",
    "$$\n",
    "(\\mathbf{I} + \\partial f)(\\mathbf{m}) \\in  \\mathbf{m} - \\mathbf{L}^T\\mathbf{z}\n",
    "$$\n",
    "\n",
    "- Invoking the definition of proximal\n",
    "\n",
    "$$\n",
    "\\mathbf{z} \\in (\\mathbf{I} + \\partial g^{\\star})^{-1}(\\mathbf{z} + \\mathbf{L}\\mathbf{m}) = prox_{g^{\\star}}(\\mathbf{z} + \\mathbf{L}\\mathbf{m})\n",
    "$$\n",
    "$$\n",
    "\\mathbf{m} \\in  (\\mathbf{I} + \\partial f)^{-1}(\\mathbf{m} - \\mathbf{L}^T\\mathbf{z}) = prox_f(\\mathbf{m} - \\mathbf{L}^T\\mathbf{z})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53f1467-6ed5-4e3f-97bd-9a23db848d6a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Indicator functions\n",
    "\n",
    "Useful to introduce hard-constraints in optimization.\n",
    "\n",
    "Definition: a function that maps elements of the subset to one, and all other elements to zero ($I: M \\rightarrow {0, 1}$)\n",
    "\n",
    "$$\n",
    "I_A(\\mathbf{m}) = [\\mathbf{m} \\in A] = \\begin{cases}\n",
    "1 \\quad \\mathbf{m} \\in A\\\\\n",
    "0 \\quad \\mathbf{m} \\notin A\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The proximal operator corresponds to its orthogonal projection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05eaf08-cb28-4795-a3db-bbfcd0c93045",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Indicator functions\n",
    "\n",
    "Examples: \n",
    "\n",
    "- Box: $\\operatorname{Box}_{[l, u]}(m) = \\{ m: l \\leq x\\leq u \\}$\n",
    "  \n",
    "- Simplex: $\\Delta_r(\\mathbf{m}) = \\{ \\mathbf{m}: \\sum_i m_i = r,\\; m_i \\geq 0 \\}$\n",
    "\n",
    "- $L_0$ Ball: $L0_r(\\mathbf{m}) = \\{ \\mathbf{m}: ||\\mathbf{m}||_0 \\leq r \\}$\n",
    "\n",
    "- $L_1$ Ball: $L1_r(\\mathbf{m}) = \\{ \\mathbf{m}: ||\\mathbf{m}||_1 \\leq r \\}$\n",
    "\n",
    "- Euclidean Ball: $\\operatorname{Eucl}_{[\\mathbf{c}, r]} = \\{ \\mathbf{m}: l ||\\mathbf{m} - \\mathbf{c}||_2 \\leq r \\}$\n",
    "\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d3d647-4fa4-4329-9a2b-fa8f8d68b22a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Indicator functions\n",
    "\n",
    "Examples: \n",
    "\n",
    "- Box: \n",
    "$$\n",
    "P_{\\operatorname{Box}_{[l, u]}} (m) = min\\{ max \\{m, l\\}, u \\} =\n",
    "        \\begin{cases}\n",
    "        l, & m < l\\\\\n",
    "        m,& l \\leq m_i \\leq u \\\\\n",
    "        u,  & m > u\\\\\n",
    "        \\end{cases}\n",
    "$$\n",
    "\n",
    "- Simplex:\n",
    "$$\n",
    "P_{Box_{[0, \\infty]} \\cap H_{\\mathbf{1}, r}} (\\mathbf{m}) = P_{Box_{[0, \\infty]}} (\\mathbf{x} - \\mu^* \\mathbf{1})\n",
    "$$\n",
    "$\\mu^*$ obtained by solving the following equation by bisection: $f(\\mu) = \\mathbf{1}^T P_{Box_{[0, \\infty]}} (\\mathbf{m} - \\mu \\mathbf{c}) - r$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b35065d-9ad1-4a95-8f90-dd4d3d062ca7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Indicator functions\n",
    "\n",
    "Examples: \n",
    "\n",
    "- $L_0$ Ball: retain the $r$ highest (in absolute value) largest entries of $\\mathbf{m}$ and zero all the other entries.\n",
    "\n",
    "- $L_1$ Ball: \n",
    "$$\n",
    "P_{L1_{r}} (\\mathbf{m}) = sign(\\mathbf{m}) P_{\\operatorname{Simplex}(r)}(\\mathbf{m})\n",
    "$$\n",
    "\n",
    "- Euclidean Ball: \n",
    "$$\n",
    "P_{\\operatorname{Eucl}_{[\\mathbf{c}, r]}} (\\mathbf{m}) = \\mathbf{c} + \\frac{r}\n",
    "        {max\\{ ||\\mathbf{m} - \\mathbf{c}||_2^2, r\\}}(\\mathbf{m} - \\mathbf{c})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b01a3-4d37-46aa-9b99-783b446fa729",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Indicator functions\n",
    "\n",
    "The algorithms that we have discussed up until now can be easily extended to include indicator functions. \n",
    "\n",
    "Examples:\n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{m}} {\\mathrm{argmin}} \\; \\|\\mathbf{m}\\|_1 \\; \\text{s.t.} \\;  \\|\\mathbf{Gm} - \\mathbf{d}\\|_2 < \\epsilon\n",
    "$$\n",
    "\n",
    "which we can equivalently write as \n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{m}} {\\mathrm{argmin}} \\; \\|\\mathbf{m}\\|_1 + i_{\\|\\mathbf{Gm} - \\mathbf{d}\\|_2 < \\epsilon}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdf14c9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Nonlinear constrained inversion\n",
    "\n",
    "The algorithms that we have discussed up until now can also be extended to nonlinear problems. \n",
    "\n",
    "Examples:\n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{m}} {\\mathrm{argmin}} \\quad f(\\mathbf{m}) \\quad s.t. \\quad \\mathbf{m}\n",
    "        \\in I_{Box}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{m}} {\\mathrm{argmin}} \\quad f(\\mathbf{m}) + \\alpha TV_i(\\mathbf{m}) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a279190-530d-439f-9946-147393c743f1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Nonlinear constrained inversion\n",
    "\n",
    "- Proximal gradient:\n",
    "\n",
    "$$\n",
    "\\mathbf{m}_k = prox_{\\delta I_{Box}} (\\mathbf{m}_{k-1} - \\nabla f(\\mathbf{m}_{k-1}))\\\\\n",
    "$$\n",
    "\n",
    "- ADMM:\n",
    "\n",
    "$$\n",
    "\\mathbf{m}_k = prox_{\\tau_k f}(\\mathbf{y}_{k-1}-\\mathbf{z}'_{k-1}) = \\underset{\\mathbf{m}} {\\mathrm{argmin}} \\quad f(\\mathbf{m}) + \\frac{1}{2\\tau_k} ||\\mathbf{m} - \\mathbf{y}_{k-1} + \\mathbf{z}'_{k-1}||_2^2\n",
    "$$\n",
    "$$\n",
    "\\mathbf{y}_k =  prox_{\\delta I_{Box}} (\\mathbf{m}_k+\\mathbf{z}'_{k-1})\n",
    "$$\n",
    "$$\n",
    "\\mathbf{z}'_k = \\mathbf{z}'_{k-1} + \\mathbf{m}_k - \\mathbf{y}_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f496e895-605a-485f-a456-52943a157085",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Nonlinear constrained inversion\n",
    "\n",
    "Time to practice: [EX5](https://github.com/mrava87/ProximalTeaching/blob/main/examples/Nonlinear.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e290c748-a91d-4c04-b47d-32b7e4380916",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Prior stacking\n",
    "\n",
    "Most of the algorithms discussed so far can be adapted to scenarios with multiple priors (i.e., multiple $g$ functions):\n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{m}} {\\mathrm{argmin}} \\quad f(\\mathbf{m}) + \\sum_i g_i(\\mathbf{m})\n",
    "$$\n",
    "\n",
    "- for ADMM: simply perform multiple splittings ($\\mathbf{y}_i=\\mathbf{m}$)\n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{m}, \\mathbf{y}_1, \\mathbf{y}_2, ..., \\mathbf{y}_N} {\\mathrm{argmin}} \\quad f(\\mathbf{m}) + \\sum_i g_i(\\mathbf{y}_i) \\; s.t. \\;  \\mathbf{y}_1=\\mathbf{m}, \\mathbf{y}_2=\\mathbf{m}, ..., \\mathbf{y}_N=\\mathbf{m}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{m}, \\mathbf{y}_1, \\mathbf{y}_2, ..., \\mathbf{y}_N} {\\mathrm{argmin}} \\underset{\\mathbf{z}_1, \\mathbf{z}_2, ..., \\mathbf{z}_N}  {\\mathrm{max}} f(\\mathbf{m}) + \\sum_i \\left[ g_i(\\mathbf{y}_i) + \\frac{\\rho_i}{2} ||\\mathbf{m}-\\mathbf{y}_i + \\mathbf{z}_i||_2^2 - \\frac{\\rho_i}{2} ||\\mathbf{z}_i||_2^2\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b083b8-7590-43cb-86b6-4599d97b7acb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Prior stacking\n",
    "\n",
    "$$\n",
    "\\mathbf{m}_k = \\underset{\\mathbf{m}} {\\mathrm{argmin}} \\quad \\mathscr{L}(\\mathbf{m}, \\mathbf{y}_{1,k-1},...,\\mathbf{y}_{N,k-1}, \\mathbf{z}_{1,k-1}, \\mathbf{z}_{N,k-1}) = ^*\n",
    "$$\n",
    "$$\n",
    "\\left\\{\n",
    "    \\begin {aligned}\n",
    "         & \\mathbf{y}_{1,k} = \\underset{\\mathbf{y}_1} {\\mathrm{argmin}} \\quad \\mathscr{L}(\\mathbf{m}_k, \\mathbf{y}_{1},...,\\mathbf{y}_{N,k-1}, \\mathbf{z}_{1,k-1}, \\mathbf{z}_{N,k-1}) = prox_{\\tau_k g_1}(\\mathbf{m}_k+\\mathbf{z}_{1,k-1})\\\\\n",
    "  & ...       \\\\\n",
    "  \\\\\n",
    "         & \\mathbf{y}_{N,k} = \\underset{\\mathbf{y}_N} {\\mathrm{argmin}} \\quad \\mathscr{L}(\\mathbf{m}_k, \\mathbf{y}_{1,k},...,\\mathbf{y}_{N}, \\mathbf{z}_{1,k-1}, \\mathbf{z}_{N,k-1}) = prox_{\\tau_k g_N}(\\mathbf{m}_k+\\mathbf{z}_{N,k-1})\\\\             \n",
    "    \\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "$$\n",
    "\\left\\{\n",
    "    \\begin {aligned}\n",
    "         & \\mathbf{z}_{1,k} = \\mathbf{z}_{1,k-1} + \\mathbf{m}_k - \\mathbf{y}_{1,k}\\\\\n",
    "  & ...       \\\\\n",
    "  \\\\\n",
    "         & \\mathbf{z}_{N,k} = \\mathbf{z}_{N,k-1} + \\mathbf{m}_k - \\mathbf{y}_{N,k}\\\\             \n",
    "    \\end{aligned}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b23a5f-69a5-42da-9fb5-22fb48138f6a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Prior stacking\n",
    "\n",
    "$^*$ when $f(\\mathbf{m}) = \\frac{1}{2} ||\\mathbf{d} - \\mathbf{Gm}||_2^2$, then:\n",
    "$$\n",
    "\\mathscr{L}=\\frac{1}{2} ||\\mathbf{d} - \\mathbf{Gm}||_2^2 + \\sum  \\frac{\\rho_i}{2} ||\\mathbf{m}-\\mathbf{y}_i + \\mathbf{z}_i||_2^2 \\rightarrow \n",
    "\\begin{bmatrix} \n",
    "\\mathbf{G} \\\\\n",
    "\\sqrt{\\rho_1}\\mathbf{I}\\\\\n",
    "...\\\\\n",
    "\\sqrt{ \\rho_N}\\mathbf{I}\\\\\n",
    "  \\end{bmatrix} \\mathbf{m} = \n",
    "\\begin{bmatrix} \n",
    "\\mathbf{d} \\\\\n",
    "\\sqrt{ \\rho_1}(\\mathbf{y}_{1} - \\mathbf{z}_{N}) \\\\\n",
    "...\\\\\n",
    "\\sqrt{ \\rho_N}(\\mathbf{y}_{N} - \\mathbf{z}_{N})\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$^*$ when $f(\\mathbf{m})$ is nonlinear, simply solve the following with numerical methods:\n",
    "$$\n",
    "\\mathscr{L}=\\frac{1}{2} f(\\mathbf{m}) + \\sum_i  \\frac{\\rho_i}{2} ||\\mathbf{m}-\\mathbf{y}_i + \\mathbf{z}_i||_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2352e94d-6b4f-4289-9207-50881ca7aeb7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Prior stacking\n",
    "\n",
    "Most of the algorithms discussed so far can be adapted to scenarios with multiple priors (i.e., multiple $g$ functions):\n",
    "\n",
    "- for ADMM with $\\mathbf{L}$/PD: \n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\begin{bmatrix} \n",
    "\\mathbf{y}_1 \\\\\n",
    "\\mathbf{y}_2 \\\\\n",
    "... \\\\\n",
    "\\mathbf{y}_N \\\\\n",
    "  \\end{bmatrix} =  \n",
    "\\begin{bmatrix} \n",
    "\\mathbf{L}_1\\mathbf{m} \\\\\n",
    "\\mathbf{L}_2\\mathbf{m} \\\\\n",
    "... \\\\\n",
    "\\mathbf{L}_N\\mathbf{m}\\\\\n",
    "  \\end{bmatrix} \\rightarrow g(\\mathbf{y}) : g_i(\\mathbf{y}_i) = g_i(\\mathbf{L}_i\\mathbf{m})\n",
    "$$\n",
    "\n",
    "and use property of proximal of sum of separable functions:\n",
    "\n",
    "$$\n",
    "prox_{\\tau g} (\\mathbf{y}) = [prox_{\\tau g_1} (\\mathbf{y}_1), prox_{\\tau g_2} (\\mathbf{y}_2), ..., prox_{\\tau g_N} (\\mathbf{y}_N)]^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a187dd-8432-4f07-b12e-8eb0fc868dae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Plug-and-Play Priors (PnP)\n",
    "\n",
    "Let's consider an alternative view of the proximal operator of a generic function $f$:\n",
    "\n",
    "$$\n",
    "prox_{\\tau g} (\\mathbf{v})= \\underset{\\mathbf{m}} {\\mathrm{argmin}} \\; \\frac{1}{2\\tau}||\\mathbf{v}-\\mathbf{m}||_2^2 + g({\\mathbf{m}})\n",
    "$$\n",
    "\n",
    "Assuming that $\\mathbf{v}$ is a noisy observation (with uncorrelated Gaussian noise), this optimization problem is equivalent to a Maximum a Posterior (MAP) denoising problem with $g$ being the regularizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8a090e-1ab9-4deb-b627-4493d8402b2b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Plug-and-Play Priors (PnP)\n",
    "\n",
    "We can see this is we define $\\mathbf{v} = \\mathbf{m} + \\mathbf{n}$ with $\\mathbf{n}\\sim \\mathcal{N}(0, \\sigma_v \\mathbf{I})$, such that:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{v}|\\mathbf{m}) \\alpha e^{-\\frac{||\\mathbf{v}-\\mathbf{m}||_2^2}{2\\sigma_v^2}},\n",
    "$$\n",
    "$$p(\\mathbf{m}) \\alpha e^{-g(\\mathbf{m})}\n",
    "$$\n",
    "\n",
    "The MAP estimator ($\\mathbf{m}_{MAP} = \\underset{\\mathbf{m}} {\\mathrm{argmax}} \\quad p(\\mathbf{v}|\\mathbf{m})p(\\mathbf{m})$) becomes:\n",
    "$$\n",
    "\\mathbf{m}_{MAP} = \\underset{\\mathbf{m}} {\\mathrm{argmin}} \\; -log p(\\mathbf{v}|\\mathbf{m}) - log p(\\mathbf{m}) = \\underset{\\mathbf{m}} {\\mathrm{argmin}} \\;  \\frac{1}{2\\sigma_v^2}||\\mathbf{v}-\\mathbf{m}||_2^2 + g(\\mathbf{m})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d5cdbc-1f3b-46ca-9818-56ad5019c067",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Plug-and-Play Priors (PnP)\n",
    "\n",
    "So we can decide to replace the proximal of a generic function $f$ with any MAP denoiser of choice (or, in practice, even a denoiser that optimizes a different criterion, e.g. MMSE)\n",
    "\n",
    "$$\n",
    "prox_{\\tau g} (\\mathbf{v})= Denoiser(\\mathbf{v}, \\tau)\n",
    "$$\n",
    "\n",
    "This is very popular nowadays with Neural Networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb49765-7049-486e-897f-b3cd3d157848",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Plug-and-Play Priors (PnP)\n",
    "\n",
    "Finally, let's also re-interpret the original problem in a Bayesian setting and look for its MAP estimator. For a linear data misfit term:\n",
    "\n",
    "$$\n",
    "\\mathbf{m}_{MAP} = \\underset{\\mathbf{m}} {\\mathrm{argmin}} \\; -log p(\\mathbf{d}|\\mathbf{m}) - log p(\\mathbf{m}) = \\underset{\\mathbf{m}} {\\mathrm{argmin}} \\;  \\frac{1}{2\\sigma_d^2} ||\\mathbf{d} - \\mathbf{Gm}||_2^2 + g(\\mathbf{m})\n",
    "$$\n",
    "\n",
    "Note that the prior can be implicitely learned by training a denoiser on a set of representative solutions of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e43c7c-dfd2-480d-b638-65da714b056e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Plug-and-Play Priors (PnP)\n",
    "\n",
    "Time to practice: [EX6](https://github.com/mrava87/ProximalTeaching/blob/main/examples/MRIPnP.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec085d64-42a3-4ddf-9cc3-3046778ab27e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "- Proximal algorithms are very powerful and versatile: they can handle all sort of smooth and non-smooth objective functions;\n",
    "\n",
    "- At the core of every proximal algorithm lie: i) gradient and ii) proximal operator of the different terms of the objective function (plus dual-proximal operator for primal-dual algorithms);\n",
    "\n",
    "- Do not use proximal algorithms if your objective function is smooth (just rely on gradient based methods, first- or second-order);\n",
    "\n",
    "- Recent connections between Bayesian formalism and proximal algorithms has shown that one can use denoisers to implicitely capture the prior probability of the solution (use denoising Neural Networks provided availability of representative training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8906a389-6abc-43ea-b0a5-1492df1c6c48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "338.790771484375px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
